{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "from summa.pagerank_weighted import pagerank_weighted_scipy as _pagerank\n",
    "from summa.preprocessing.textcleaner import clean_text_by_sentences as _clean_text_by_sentences\n",
    "from summa.commons import build_graph as _build_graph\n",
    "from summa.commons import remove_unreachable_nodes as _remove_unreachable_nodes\n",
    "from summa.keywords import keywords\n",
    "from summa.pagerank_weighted import pagerank_weighted as pagerank\n",
    "from topmine_src import phrase_mining\n",
    "from PyRouge.pyrouge import Rouge\n",
    "from bm25 import get_bm25_weights as _bm25_weights\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading and Phrase Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = ['1.txt', '2.txt', '3.txt', '4.txt', '5.txt', '6.txt', '7.txt', '8.txt', '9.txt', \n",
    "#              '10.txt', '11.txt', '12.txt', '13.txt', '14.txt', '15.txt']\n",
    "file_name = ['1.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_extract(file_name):\n",
    "    # represents the minimum number of occurences you want each phrase to have.\n",
    "    min_support=2\n",
    "\n",
    "    # represents the threshold for merging two words into a phrase. A lower value\n",
    "    # alpha leads to higher recall and lower precision,\n",
    "    alpha=4\n",
    "\n",
    "    # length of the maximum phrase size\n",
    "    max_phrase_size=10\n",
    "\n",
    "    phrase_miner = phrase_mining.PhraseMining(file_name, min_support, max_phrase_size, alpha);\n",
    "    partitioned_docs, index_vocab = phrase_miner.mine()\n",
    "    frequent_phrases = phrase_miner.get_frequent_phrases(min_support)\n",
    "    \n",
    "    return frequent_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_graph_edge_weights(graph):\n",
    "    WEIGHT_THRESHOLD = 1.e-3\n",
    "    weights = _bm25_weights(graph.nodes())\n",
    "    i = 0\n",
    "    for sentence_1 in graph.nodes():\n",
    "        j = 0\n",
    "        for sentence_2 in graph.nodes():\n",
    "            weight = (weights[i][j] + 100) * 0.1\n",
    "            if i == j or weight < WEIGHT_THRESHOLD:\n",
    "                continue\n",
    "\n",
    "            edge = (sentence_1, sentence_2)\n",
    "            if sentence_1 != sentence_2 and not graph.has_edge(edge):\n",
    "                    graph.add_edge(edge, weight * graph.node_weight[sentence_2])\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Handles the case in which all similarities are zero.\n",
    "    # The resultant summary will consist of random sentences.\n",
    "    if all(graph.edge_weight(edge) == 0 for edge in graph.edges()):\n",
    "        _create_valid_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_valid_graph(graph):\n",
    "    nodes = graph.nodes()\n",
    "\n",
    "    for i in range(len(nodes)):\n",
    "        for j in range(len(nodes)):\n",
    "            if i == j:\n",
    "                continue\n",
    "\n",
    "            edge = (nodes[i], nodes[j])\n",
    "\n",
    "            if graph.has_edge(edge):\n",
    "                graph.del_edge(edge)\n",
    "\n",
    "            graph.add_edge(edge, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_similarity(s1, s2):\n",
    "    words_sentence_one = s1.split()\n",
    "    words_sentence_two = s2.split()\n",
    "\n",
    "    common_word_count = len(set(words_sentence_one) & set(words_sentence_two))\n",
    "\n",
    "    log_s1 = log10(len(words_sentence_one))\n",
    "    log_s2 = log10(len(words_sentence_two))\n",
    "\n",
    "    if log_s1 + log_s2 == 0:\n",
    "        return 0\n",
    "\n",
    "    return common_word_count / (log_s1 + log_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_results(extracted_sentences, split, score):\n",
    "    if score:\n",
    "        return [(sentence.text, sentence.score) for sentence in extracted_sentences]\n",
    "    if split:\n",
    "        return [sentence.text for sentence in extracted_sentences]\n",
    "    return \" \".join([sentence.text for sentence in extracted_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sentences_with_word_count(sentences, words):\n",
    "    \"\"\" Given a list of sentences, returns a list of sentences with a\n",
    "    total word count similar to the word count provided.\n",
    "    \"\"\"\n",
    "    word_count = 0\n",
    "    selected_sentences = []\n",
    "    # Loops until the word count is reached.\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(sentence.text.split())\n",
    "\n",
    "        # Checks if the inclusion of the sentence gives a better approximation\n",
    "        # to the word parameter.\n",
    "        if abs(words - word_count - words_in_sentence) > abs(words - word_count):\n",
    "            return selected_sentences\n",
    "\n",
    "        selected_sentences.append(sentence)\n",
    "        word_count += words_in_sentence\n",
    "\n",
    "    return selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_most_important_sentences(sentences, ratio, words):\n",
    "    sentences.sort(key=lambda s: s.score, reverse=True)\n",
    "\n",
    "    # If no \"words\" option is selected, the number of sentences is\n",
    "    # reduced by the provided ratio.\n",
    "    if words is None:\n",
    "        length = len(sentences) * ratio\n",
    "        return sentences[:int(length)]\n",
    "\n",
    "    # Else, the ratio is ignored.\n",
    "    else:\n",
    "        return _get_sentences_with_word_count(sentences, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(text, language=\"english\"):\n",
    "    sentences = _clean_text_by_sentences(text, language)\n",
    "\n",
    "    graph = _build_graph([sentence.token for sentence in sentences])\n",
    "    _set_graph_edge_weights(graph)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weights to sentecences containing important phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_scores_to_sentences(sentences, scores, frequent_phrases):\n",
    "    for sentence in sentences:\n",
    "        # Adds the score to the object if it has one.\n",
    "        if sentence.token in scores:\n",
    "            sentence.score = scores[sentence.token]\n",
    "        else:\n",
    "            sentence.score = 0\n",
    "            \n",
    "#         for phrase in frequent_phrases:\n",
    "#             if((sentence.text.lower()).find(phrase) >= 0):\n",
    "#                 sentence.score += frequent_phrases[phrase] * 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(file):\n",
    "    with open ('../data/document/' + file_name[0], 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    text = data.replace('\\n', '')\n",
    "    \n",
    "    frequent_phrases = phrase_extract('../data/document/' + file)\n",
    "    \n",
    "    ratio=0.2\n",
    "    words=None\n",
    "    split=False\n",
    "    scores=False\n",
    "    additional_stopwords=None\n",
    "\n",
    "    # Gets a list of processed sentences.\n",
    "    sentences = _clean_text_by_sentences(text, 'english', additional_stopwords)\n",
    "\n",
    "    # Creates the graph and calculates the similarity coefficient for every pair of nodes.\n",
    "#     graph = _build_graph([sentence.token for sentence in sentences])\n",
    "    graph = _build_graph(sentences, frequent_phrases)\n",
    "    _set_graph_edge_weights(graph)\n",
    "\n",
    "    # Remove all nodes with all edges weights equal to zero.\n",
    "    _remove_unreachable_nodes(graph)\n",
    "\n",
    "    # PageRank cannot be run in an empty graph.\n",
    "    if len(graph.nodes()) == 0:\n",
    "        print('Graph is Empty')\n",
    "\n",
    "    # Ranks the tokens using the PageRank algorithm. Returns dict of sentence -> score\n",
    "    pagerank_scores = _pagerank(graph)\n",
    "\n",
    "    # Adds the summa scores to the sentence objects.\n",
    "    _add_scores_to_sentences(sentences, pagerank_scores, frequent_phrases)\n",
    "\n",
    "    # Extracts the most important sentences with the selected criterion.\n",
    "    extracted_sentences = _extract_most_important_sentences(sentences, ratio, words)\n",
    "\n",
    "    # Sorts the extracted sentences by apparition order in the original text.\n",
    "    extracted_sentences.sort(key=lambda s: s.index)\n",
    "\n",
    "    res = _format_results(extracted_sentences, split, scores)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['However, only about 25 to 30% of the homes in the earthquake-prone San Francisco area have earthquake insurance. The damage goes far beyond that of Hurricane Hugo along the Carolinas last month. About 25% of the policyholders have earthquake insurance.']\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for file in file_name:\n",
    "    summary = get_summary(file)\n",
    "    res.append(summary)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The White House and the California government are provided aid to Tuesday's California earthquake victims. Only 25-30% of homes in the San Francisco area are insured. Insurers may have to pay billions but no company is expected to suffer financial damage. Insurers price-cutting may end as prices rise from reinsurance investments.\"]\n"
     ]
    }
   ],
   "source": [
    "refer = []\n",
    "for file in file_name:\n",
    "    with open ('../data/summary/' + file, 'r', encoding='utf-8') as f:\n",
    "        summary = f.read().replace('\\n','')\n",
    "    refer.append(summary)\n",
    "print(refer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5158730158730159\n",
      "0.39274924471299094\n",
      "0.44597007613250755\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "precision = 0\n",
    "recall = 0\n",
    "f_score = 0\n",
    "for i in range(len(file_name)):\n",
    "    [p, r, f] = rouge.rouge_l([res[i]], [refer[i]])\n",
    "    precision += p\n",
    "    recall += r\n",
    "    f_score += f\n",
    "\n",
    "print(precision / len(file_name))\n",
    "print(recall / len(file_name))\n",
    "print(f_score / len(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
